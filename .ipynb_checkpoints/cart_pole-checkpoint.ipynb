{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c267b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/maciejbalawejder/Reinforcement-Learning-Collection/blob/main/Q-Table/Qtable.ipynb\n",
    "# https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "# https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd46f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns Q-table\n",
    "def Qtable(state_space,action_space,bin_size = 30):\n",
    "    \n",
    "    bins = [np.linspace(-4.8,4.8,bin_size),\n",
    "            np.linspace(-4,4,bin_size),\n",
    "            np.linspace(-0.418,0.418,bin_size),\n",
    "            np.linspace(-4,4,bin_size)]\n",
    "    \n",
    "    q_table = np.random.uniform(low=-1,high=1,size=([bin_size] * state_space + [action_space]))\n",
    "    return q_table, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bd53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): index.append(np.digitize(state[i],bins[i]) - 1)\n",
    "    return tuple(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d321f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env,q_table, bins,reward_array, episodes = 5000, gamma = 0.95, lr = 0.1, timestep = 100, epsilon = 0.2):\n",
    "    \n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    solved = 0\n",
    "    for episode in range(1,episodes+1):\n",
    "        steps += 1 \n",
    "        # env.reset() => initial observation\n",
    "        current_state = Discrete(env.reset()[0],bins)\n",
    "      \n",
    "        score = 0\n",
    "        terminated = False\n",
    "        while not terminated: \n",
    "            #if episode%timestep==0: env.render()\n",
    "\n",
    "            # Exploration\n",
    "            if np.random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[current_state])\n",
    "            \n",
    "            # Take step\n",
    "            observation, reward, terminated, truncated, info  = env.step(action)\n",
    "            next_state = Discrete(observation,bins)\n",
    "            score+=reward\n",
    "          \n",
    "            #Update Q-table\n",
    "            if not terminated:\n",
    "                    max_future_q = np.max(q_table[next_state])\n",
    "                    current_q = q_table[current_state+(action,)]\n",
    "                    new_q = (1-lr)*current_q + lr*(reward + gamma*max_future_q)\n",
    "                    q_table[current_state+(action,)] = new_q\n",
    "            current_state = next_state\n",
    "            \n",
    "        # End of the loop update\n",
    "        else:\n",
    "            rewards += score\n",
    "            reward_array.append(rewards)\n",
    "            \n",
    "            #print results\n",
    "            clear_output(wait=True)\n",
    "            if score > 195 and steps >= 100: solved += 1\n",
    "            print(episode, \"/\", episodes, \"Episodes done,\", str(np.round(solved/episode,4))+\"% solved\")\n",
    "        if episode % timestep == 0: print(reward / timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b23c04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, q_table, bins):\n",
    "    # env.reset() => initial observation\n",
    "    current_state = Discrete(env.reset()[0],bins)\n",
    "    score = 0\n",
    "\n",
    "    for _ in range(1000):\n",
    "        # take action based on q-table\n",
    "        action = np.argmax(q_table[current_state])\n",
    "        # take next step\n",
    "        observation, reward, terminated, truncated, info  = env.step(action)\n",
    "        next_state = Discrete(observation,bins)\n",
    "\n",
    "        score+=reward\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        if terminated: return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc30eba",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fed5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make(\"CartPole-v1\")\n",
    "train_env.reset()\n",
    "\n",
    "# stats\n",
    "print(train_env.action_space) # 2 actions\n",
    "print(train_env.observation_space) # 4 states\n",
    "# Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
    "# [x_min, vx_min, phi_min, vphi_min][x_max, vx_max, phi_max, vphi_max]\n",
    "\n",
    "### Q-Table\n",
    "q_table, bins = Qtable(len(train_env.observation_space.low), train_env.action_space.n)\n",
    "print(q_table.shape)\n",
    "\n",
    "### Training\n",
    "reward_array = []\n",
    "Q_learning(train_env,q_table, bins, reward_array, lr = 0.25, gamma = 0.995, episodes = 1*10**4, timestep = 1000)\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63909583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cf41381",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eba17fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "test_env.reset()\n",
    "test(test_env,q_table,bins)\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b17bf7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4fa77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
